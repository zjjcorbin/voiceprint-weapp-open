#!/usr/bin/env python3
"""
æµ‹è¯•æ¨¡åž‹å¯ç”¨æ€§å’ŒåŸºæœ¬åŠŸèƒ½
"""

import torch
import tempfile
import numpy as np
import soundfile as sf
import os
import sys
from loguru import logger
from app.core.config import settings

def create_test_audio():
    """åˆ›å»ºæµ‹è¯•éŸ³é¢‘æ–‡ä»¶"""
    # ç”Ÿæˆç®€å•çš„æ­£å¼¦æ³¢éŸ³é¢‘
    sample_rate = 16000
    duration = 3  # 3ç§’
    frequency = 440  # A4éŸ³ç¬¦
    
    t = np.linspace(0, duration, int(sample_rate * duration))
    audio = np.sin(2 * np.pi * frequency * t) * 0.5  # 50%éŸ³é‡
    
    return audio, sample_rate

def test_speaker_recognition():
    """æµ‹è¯•å£°çº¹è¯†åˆ«æ¨¡åž‹"""
    print("æµ‹è¯•å£°çº¹è¯†åˆ«æ¨¡åž‹...")
    
    try:
        try:
            from speechbrain.inference.speaker import SpeakerRecognition
        except ImportError:
            from speechbrain.pretrained import SpeakerRecognition
        
        # åŠ è½½æ¨¡åž‹
        model = SpeakerRecognition.from_hparams(
            source="speechbrain/spkrec-ecapa-voxceleb",
            savedir="pretrained_models/spkrec-ecapa-voxceleb"
        )
        
        # åˆ›å»ºæµ‹è¯•éŸ³é¢‘
        audio, sr = create_test_audio()
        audio_tensor = torch.tensor(audio).unsqueeze(0).float()
        
        # æå–å£°çº¹ç‰¹å¾
        with torch.no_grad():
            embedding = model.encode_batch(audio_tensor)
            
        print(f"âœ“ å£°çº¹ç‰¹å¾æå–æˆåŠŸï¼Œç‰¹å¾ç»´åº¦: {embedding.shape}")
        return True
        
    except Exception as e:
        print(f"âœ— å£°çº¹è¯†åˆ«æ¨¡åž‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_emotion_recognition():
    """æµ‹è¯•æƒ…ç»ªè¯†åˆ«æ¨¡åž‹ - ä½¿ç”¨ HuggingFace AutoFeatureExtractor + AutoModel"""
    print("æµ‹è¯•æƒ…ç»ªè¯†åˆ«æ¨¡åž‹...")
    
    model_name = settings.EMOTION_MODEL
    
    print(f"  æµ‹è¯•æ¨¡åž‹: {model_name}")
    try:
        from transformers import AutoFeatureExtractor, AutoModelForSequenceClassification
        import torch
        
        # åŠ è½½ç‰¹å¾æå–å™¨å’Œæ¨¡åž‹
        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        
        # åˆ›å»ºæµ‹è¯•éŸ³é¢‘
        audio, sr = create_test_audio()
        
        # é¢„å¤„ç†éŸ³é¢‘ï¼ˆä½¿ç”¨ feature_extractorï¼‰
        inputs = feature_extractor(audio, sampling_rate=sr, return_tensors="pt", padding=True)
        
        # æŽ¨ç†
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            score, index = torch.max(probs, dim=-1)
            emotion_label = model.config.id2label[index.item()]
            confidence = score.item()
        
        print(f"  âœ“ æƒ…ç»ªè¯†åˆ«æ¨¡åž‹ {model_name} åŠ è½½æˆåŠŸ")
        print(f"    é¢„æµ‹æƒ…ç»ª: {emotion_label}, ç½®ä¿¡åº¦: {confidence:.4f}")
        return True
        
    except Exception as e:
        print(f"  âœ— æ¨¡åž‹ {model_name} æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_audio_processing():
    """æµ‹è¯•éŸ³é¢‘å¤„ç†åŠŸèƒ½"""
    print("æµ‹è¯•éŸ³é¢‘å¤„ç†åŠŸèƒ½...")
    
    try:
        import librosa
        import soundfile as sf
        
        # åˆ›å»ºæµ‹è¯•éŸ³é¢‘
        audio, sr = create_test_audio()
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
            sf.write(tmp_file.name, audio, sr)
            
            # ä½¿ç”¨librosaåŠ è½½
            y, sr_loaded = librosa.load(tmp_file.name, sr=16000)
            
            # éªŒè¯éŸ³é¢‘
            assert sr_loaded == 16000, "é‡‡æ ·çŽ‡ä¸åŒ¹é…"
            assert len(y) > 0, "éŸ³é¢‘æ•°æ®ä¸ºç©º"
            
            print(f"âœ“ éŸ³é¢‘å¤„ç†æµ‹è¯•é€šè¿‡")
            print(f"  é‡‡æ ·çŽ‡: {sr_loaded}")
            print(f"  éŸ³é¢‘é•¿åº¦: {len(y)} æ ·æœ¬")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp_file.name)
            return True
            
    except Exception as e:
        print(f"âœ— éŸ³é¢‘å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("æ¨¡åž‹åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥PyTorch
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    
    print("\nå¼€å§‹æµ‹è¯•...")
    
    results = {}
    
    # æµ‹è¯•éŸ³é¢‘å¤„ç†
    results['audio_processing'] = test_audio_processing()
    
    # æµ‹è¯•å£°çº¹è¯†åˆ«
    results['speaker_recognition'] = test_speaker_recognition()
    
    # æµ‹è¯•æƒ…ç»ªè¯†åˆ«
    results['emotion_recognition'] = test_emotion_recognition()
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ€»ç»“:")
    
    for test_name, result in results.items():
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    success_count = sum(results.values())
    total_count = len(results)
    
    print(f"\næ€»ä½“ç»“æžœ: {success_count}/{total_count} æµ‹è¯•é€šè¿‡")
    
    if success_count == total_count:
        print("ðŸŽ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸è¿è¡Œã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œç³»ç»ŸåŠŸèƒ½å¯èƒ½å—é™ã€‚")
    
    return success_count > 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)